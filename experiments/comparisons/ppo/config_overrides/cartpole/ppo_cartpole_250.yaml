algo: ppo
algo_config:
  encode_prior: True # if to learn with prior
  prior_info:
    prior_prop:
      cart_mass: 1.0
      pole_length: 0.5
      pole_mass: 0.1
    # translate model prior as linear MPC
    horizon: 20
    q_mpc:
    - 1
    - 1
    - 1
    - 1
    r_mpc:
    - 0.1
    warmstart: true
    soft_constraints:
      prior_soft_constraints: true
      prior_soft_constraints_coeff: 10
    prior_param_coeff: 2.5
  # model args
  hidden_dim: 64
  activation: "relu"
  norm_obs: False
  norm_reward: False
  clip_obs: 10.0
  clip_reward: 10.0

  # loss args
  gamma: 0.99
  use_gae: True
  gae_lambda: 0.95
  use_clipped_value: True
  clip_param: 0.2
  target_kl: 0.01
  entropy_coef: 0.01

  # prior encodeing
  discount_steps: 10
  improving_factor: 1.3
  breaking_steps: 10000
  eval_every: 5

  # optim args
  opt_epochs: 10
  mini_batch_size: 32 # 64
  actor_lr: 0.0003
  critic_lr: 0.001
  max_grad_norm: 0.5

  # runner args
  max_env_steps: 72000
  num_workers: 1
  rollout_batch_size: 4
  rollout_steps: 100
  deque_size: 10
  eval_batch_size: 10

  # misc
  log_interval: 72000
  save_interval: 72000
  eval_interval: 72000
  num_checkpoints: 50
  eval_save_best: True
  tensorboard: False
